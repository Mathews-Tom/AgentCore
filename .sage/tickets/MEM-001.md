# MEM-001: Memory Service with Four-Layer Architecture + Mem0

**State:** UNPROCESSED
**Priority:** P0
**Type:** Epic
**Component:** memory-service
**Effort:** 20-25 days (3-4 weeks)
**Phase:** 2

## Description

Implement a comprehensive memory service combining a four-layer memory architecture with Mem0's universal memory infrastructure. This hybrid approach leverages structured memory layers with powerful semantic search capabilities.

**Four-Layer Architecture:**
- **Working Memory**: Immediate context (Redis, 2-4K tokens, 1-hour TTL)
- **Episodic Memory**: Recent conversation episodes (Mem0/Qdrant, 50 episodes)
- **Semantic Memory**: Long-term facts and knowledge (Mem0/Qdrant, 1000+ entries)
- **Procedural Memory**: Action-outcome patterns (Mem0/Qdrant, workflows)

**Mem0 Integration:**
- Vector-based semantic search across all memory layers
- Intelligent memory extraction and encoding
- Cross-session memory continuity
- Task artifact storage with retrieval
- +26% accuracy improvement (per Mem0 benchmarks)

**Expected Impact:**
- 80% reduction in context tokens for long sessions
- 25-30% improvement in multi-turn task success rates
- <100ms retrieval latency at scale (1M+ memories)
- 90%+ retrieval precision in semantic search
- <5% contradictory retrievals (memory coherence)

## Acceptance Criteria

### Four-Layer Architecture
- [ ] Working Memory implemented in Redis (2-4K tokens, 1-hour TTL, <10ms access)
- [ ] Episodic Memory implemented in Mem0/Qdrant (50 episodes, temporal context)
- [ ] Semantic Memory implemented in Mem0/Qdrant (1000+ entries, pruning logic)
- [ ] Procedural Memory implemented in Mem0/Qdrant (action patterns, success tracking)
- [ ] Memory transitions working (working → episodic → semantic)

### Mem0 Integration
- [ ] Mem0 integrated with Qdrant vector database
- [ ] Semantic search operational across all memory layers
- [ ] Memory extraction and encoding working (summarization, entity extraction)
- [ ] Hybrid retrieval scoring (40% relevance + 30% recency + 30% frequency)

### Performance & Quality
- [ ] Context efficiency: 80%+ reduction in context tokens
- [ ] Retrieval precision: 90%+ relevant memories in semantic search
- [ ] Task performance: +25-30% success rate on multi-turn tasks
- [ ] Memory coherence: <5% contradictory retrievals
- [ ] Retrieval latency: <100ms p95
- [ ] Working memory access: <10ms p95
- [ ] Storage scalability: 1M+ memories per agent without degradation

### Integration & Testing
- [ ] All JSON-RPC methods functional (memory.add, memory.search, memory.retrieve, memory.transition)
- [ ] SessionManager integration complete
- [ ] MessageRouter context-aware routing functional
- [ ] TaskManager artifact storage operational
- [ ] Unit tests achieving 90%+ coverage
- [ ] Integration tests across all memory layers
- [ ] Load tests demonstrating performance targets

## Dependencies

### External Dependencies (Updated from Plan)
- **Mem0** (mem0 ^0.1.0) - Memory extraction, fact recognition, entity detection
- **PGVector** (pgvector >= 0.4.1) - Vector storage (ALREADY IN DEPENDENCIES - using instead of Qdrant)
- **OpenAI API** (optional) - Embeddings for critical memories (text-embedding-3-small)
- **SentenceTransformers** (sentence-transformers >= 5.1.1) - Local embeddings (ALREADY IN DEPENDENCIES)
- **LLM API** (gpt-4.1-mini) - Compression via test-time scaling

### Infrastructure Requirements (Revised)
- **PostgreSQL 14+ with PGVector**: Existing AgentCore database (A2A-009)
  - PGVector extension enabled
  - IVFFlat index for vector similarity search
  - Storage: 10-20GB for 1M memories
- **Redis 6+**: Existing AgentCore Redis cluster for working memory
  - Additional DB: REDIS_MEMORY_DB=1
  - Memory: ~500MB for memory cache
  - TTL: 1 hour for working memory

### Component Dependencies (Blocking)
- **A2A-009 (PostgreSQL Integration)**: CRITICAL blocker - database schema required
- **A2A-004 (Task Management)**: CRITICAL blocker - task_id references required
- **A2A-002 (JSON-RPC Core)**: CRITICAL blocker - method registration required

### Component Dependencies (Parallel Integration)
- **A2A-005 (Message Routing)**: Memory-aware routing integrates with memory service
- **A2A-019 to A2A-021 (Session Management)**: Session context uses memory service
- **A2A-003 (Agent Card Management)**: Agent preferences stored in semantic memory

### Component Dependencies (Future Consumers)
- **ART-014 (Agent State Persistence)**: Agent runtime will query memory for context
- **ART-005 (Multi-Agent Coordination)**: Coordination uses procedural memory patterns
- **ART-013 (Error Handling)**: Error tracking integrates with memory service

## Context

**Specification:** `docs/specs/memory-service/spec.md` + `docs/specs/memory-system/spec.md` (COMPASS-enhanced)
**Implementation Plan:** `docs/specs/memory-service/plan.md` (PRP format with architecture decisions)
**Source Research:**
- `docs/research/evolving-memory-system.md` (four-layer architecture foundation)
- COMPASS paper analysis (stage-aware context management, test-time scaling)

**Traceability Chain:**
Research (evolving-memory-system.md) → Specification (memory-service/spec.md) → COMPASS Enhancement (memory-system/spec.md) → Implementation Plan (memory-service/plan.md)

**Revised Implementation Phases (9 weeks with COMPASS enhancements):**
1. **Weeks 1-2: Foundation** - Database schema, PGVector setup, models, basic memory operations, encoding service
2. **Week 3: Core Operations** - Vector search, retrieval, JSON-RPC methods, integration with Task/Session managers
3. **Weeks 4-5: COMPASS Stage Management** - Stage detection, transitions, progressive compression (10:1, 5:1), cost tracking
4. **Week 6: COMPASS Error Tracking** - Error recording, pattern detection, error-aware retrieval, ACE signals
5. **Week 7: Enhanced Retrieval** - Multi-factor scoring (6 factors), critical memory identification, stage-aware retrieval
6. **Weeks 8-9: Integration & Optimization** - MessageRouter integration, ACE queries, performance tuning, comprehensive testing

## Architecture (Updated from Plan)

**Architecture Pattern:** Layered Service Architecture with Four-Layer Memory + COMPASS Enhancements

**Key Design Decisions (from `plan.md`):**

**ADR-001: PGVector instead of Qdrant**
- **Decision:** Use PGVector (existing infrastructure) instead of Qdrant for vector storage
- **Rationale:** Reduces operational complexity, already in AgentCore dependencies (pgvector>=0.4.1), sufficient performance for 1M vectors
- **Trade-off:** Qdrant has better performance at 10M+ scale, but AgentCore targets 1M memories per agent
- **Fallback:** Can migrate to Qdrant later if needed (compatible semantic search interface)

**ADR-002: Hybrid Embeddings (Local + Cloud)**
- **Decision:** Use local SentenceTransformers by default (768 dims), OpenAI for critical memories only (1536 dims)
- **Rationale:** Cost optimization (70-80% savings) while maintaining quality for important content
- **Trade-off:** Slight quality reduction for non-critical memories
- **Validation:** A/B test retrieval quality with hybrid vs full OpenAI

**ADR-003: Test-Time Scaling for Compression (COMPASS)**
- **Decision:** Use gpt-4.1-mini for compression, gpt-4.1/gpt-5 for agent reasoning
- **Rationale:** COMPASS paper validated 70-80% cost reduction with maintained quality
- **Expected Savings:** 70-80% reduction in LLM API costs
- **Validation:** Track compression quality (≥95% fact retention target)

**ADR-004: Four-Layer Memory Architecture**
- **Working:** Redis cache (2-4K tokens, 1-hour TTL) - ultra-fast access
- **Episodic:** PGVector (50 recent episodes) - temporal context preservation
- **Semantic:** PGVector (1000+ facts with pruning) - long-term knowledge
- **Procedural:** PGVector (action-outcome patterns) - learned behaviors
- **Rationale:** Research validated this structure for bounded context with unbounded memory
- **Validation:** Measure context reduction (target: 80%+) and task performance (target: +20%)

**ADR-005: COMPASS Stage-Aware Context Management**
- **Decision:** Hierarchical organization (raw → stage → task) with progressive compression
- **Rationale:** COMPASS paper validated 20% accuracy improvement, 60-80% context reduction
- **Implementation:** Stage detection, 10:1 stage compression, 5:1 task compression
- **Validation:** Track fact retention (≥95%), compression ratios, cost savings

**ADR-006: Multi-Factor Importance Scoring (6 Factors)**
- **Embedding similarity:** 35% weight (PGVector cosine distance)
- **Recency:** 15% weight (exponential decay)
- **Frequency:** 10% weight (log access count)
- **Stage relevance:** 20% weight (current stage match)
- **Criticality boost:** 10% weight (2x multiplier)
- **Error correction:** 10% weight (recent error relevance)
- **Target:** 95% retrieval precision (improved from 90% baseline)

**ADR-007: Agent Isolation & Security**
- Scope-based security via agent_id filtering
- All queries filtered by agent_id/session_id/user_id
- No cross-agent memory access (enforced at service level)
- JWT authentication via A2A protocol

**Component Structure:**
```
src/agentcore/a2a_protocol/
├── models/
│   └── memory.py                  # MemoryLayer enum, MemoryEntry, MemorySearchQuery,
│                                  # WorkingMemory, EpisodicMemory, SemanticMemory, ProceduralMemory
├── services/
│   ├── memory/
│   │   ├── manager.py             # MemoryManager (orchestration, layer transitions)
│   │   ├── working_memory.py      # WorkingMemoryService (Redis cache)
│   │   ├── episodic_memory.py     # EpisodicMemoryService (Mem0/Qdrant)
│   │   ├── semantic_memory.py     # SemanticMemoryService (Mem0/Qdrant)
│   │   ├── procedural_memory.py   # ProceduralMemoryService (Mem0/Qdrant)
│   │   ├── encoding.py            # EncodingService (summarization, entity extraction)
│   │   ├── retrieval.py           # RetrievalService (hybrid search, scoring)
│   │   ├── compression.py         # CompressionService (token counting, summarization)
│   │   └── transitions.py         # TransitionService (layer promotion logic)
│   └── memory_jsonrpc.py          # JSON-RPC methods (memory.*)
├── cache/
│   └── memory_cache.py            # Redis cache adapter for working memory
└── database/
    ├── models.py                  # SQLAlchemy models (MemoryModel with layer field)
    └── repositories.py            # MemoryRepository (CRUD, vector search, layer filtering)
```

## Technology Stack (Updated from Plan)

| Component | Technology | Version | Rationale | Status |
|-----------|------------|---------|-----------|--------|
| Memory Extraction | Mem0 | 0.1.0+ | Entity recognition, fact extraction | **Need to add** |
| Vector DB | PGVector | 0.4.1+ | Vector storage (instead of Qdrant) | ✅ **Already in deps** |
| Embeddings (Local) | SentenceTransformers | 5.1.1+ | Free, 768 dims, default for cost savings | ✅ **Already in deps** |
| Embeddings (Cloud) | OpenAI | text-embedding-3-small | 1536 dims, critical memories only | Optional |
| Compression Model | OpenAI | gpt-4.1-mini | Test-time scaling, $0.15/1M tokens | Per CLAUDE.md |
| Working Memory | Redis | 6+ | Existing cluster, TTL support | ✅ **Deployed** |
| Metadata DB | PostgreSQL | 14+ | Existing database with PGVector | ✅ **Deployed (A2A-009)** |
| ORM | SQLAlchemy | 2.0+ async | Existing AgentCore pattern | ✅ **In use** |
| Validation | Pydantic | 2.5+ | Existing AgentCore pattern | ✅ **In use** |
| Framework | FastAPI | 0.104.0+ | Existing AgentCore infrastructure | ✅ **In use** |
| Testing | pytest + testcontainers | Latest | Existing test infrastructure | ✅ **In use** |

**Key Changes from Original Spec:**
- ❌ **Removed Qdrant** - Using PGVector instead (already deployed, simpler operations)
- ✅ **Added Hybrid Embeddings** - Local (free) + OpenAI (critical only) for cost optimization
- ✅ **Added Compression Model** - gpt-4.1-mini for test-time scaling per COMPASS
- ✅ **Leveraged Existing Infrastructure** - PostgreSQL, Redis, SentenceTransformers already available

**Integration Points:**
- Extends `src/agentcore/a2a_protocol/database/connection.py` session management
- Follows `src/agentcore/a2a_protocol/services/task_jsonrpc.py` JSON-RPC pattern
- Integrates with SessionManager, AgentManager, MessageRouter, TaskManager
- ACE integration for meta-cognitive monitoring (COMPASS enhancement)

## Risks & Mitigations (Updated from Plan)

**Critical Risks:**
1. **PGVector Performance at Scale** (Impact: HIGH, Likelihood: MEDIUM)
   - Mitigation: IVFFlat index with proper list count (sqrt(total_vectors)), query optimization (LIMIT, early termination)
   - Fallback: Migration to Qdrant if needed (compatible semantic search interface)
   - Validation: Benchmark with 100K vectors in Phase 1

2. **Mem0 Library Stability** (Impact: MEDIUM, Likelihood: MEDIUM)
   - Mitigation: Pin exact version `mem0==0.1.x`, abstract behind MemoryExtractor protocol
   - Fallback: Custom LLM extraction if library breaks
   - Validation: Comprehensive integration tests

3. **Cost Overrun from Embeddings** (Impact: HIGH, Likelihood: LOW)
   - Mitigation: Default to local SentenceTransformers, OpenAI only for critical memories
   - Monitoring: Track costs, alert at 75% budget
   - Validation: Cost tracking dashboard

4. **Compression Quality Degradation** (Impact: HIGH, Likelihood: MEDIUM)
   - Mitigation: Validate compression (≥95% fact retention), preserve originals as backup, adaptive compression
   - Validation: Quality metrics after each compression

5. **Dependency on A2A-009** (Impact: CRITICAL, Likelihood: LOW)
   - Mitigation: Verify A2A-009 status before starting, request priority if blocked
   - Parallel: Develop with local PostgreSQL
   - Validation: Check ticket status

6. **ACE Integration Complexity** (Impact: MEDIUM, Likelihood: MEDIUM)
   - Mitigation: Design ACE interface early, mock ACE for testing, make optional (Phase 6)
   - Validation: Stub ACE interface in Phase 1

7. **Stage Detection Accuracy** (Impact: MEDIUM, Likelihood: MEDIUM)
   - Mitigation: Multiple detection strategies, manual stage markers, validate ≥90% accuracy
   - Validation: A/B test stage detection algorithms

8. **Token Budget Exhaustion** (Impact: HIGH, Likelihood: LOW)
   - Mitigation: Hard cap at monthly budget, alert at 75%, fallback: disable compression, use local embeddings
   - Validation: Monthly cost reports

## Progress

**Status:** ✅ **Plan Complete** - Ready for `/sage.tasks MEM-001`
**Plan Generated:** 2025-10-25
**Plan Location:** `docs/specs/memory-service/plan.md` (PRP format, 100% traceability)

**Blocked By:**
- **A2A-009 (PostgreSQL Integration)**: CRITICAL - database schema required
- **A2A-004 (Task Management)**: CRITICAL - task_id references required
- **A2A-002 (JSON-RPC Core)**: CRITICAL - method registration required

**Planning Complete:**
- ✅ Comprehensive context assembly (6 priority levels)
- ✅ Research → Spec → Enhancement → Plan traceability chain established
- ✅ Four-layer architecture + COMPASS enhancements integrated
- ✅ Technology stack aligned with existing AgentCore infrastructure (PGVector, Redis, SentenceTransformers)
- ✅ Architecture decisions documented (7 ADRs)
- ✅ 9-week realistic timeline with COMPASS enhancements
- ✅ Cross-component dependencies mapped (blocking, parallel, future)
- ✅ 8 critical risks identified with mitigation strategies
- ✅ Epic ticket updated with architecture context

**Key Decisions from Plan:**
1. **ADR-001**: PGVector instead of Qdrant (simpler operations, already deployed)
2. **ADR-002**: Hybrid embeddings (local + cloud for cost optimization)
3. **ADR-003**: Test-time scaling compression (70-80% cost reduction)
4. **ADR-005**: COMPASS stage-aware context management (20% accuracy improvement)
5. **ADR-006**: Multi-factor importance scoring (6 factors, 95% precision target)

**Next Steps:**
1. ✅ **DONE**: `/sage.plan MEM` - Generated comprehensive PRP-format implementation plan
2. **NEXT**: `/sage.tasks memory-service` - Generate SMART task breakdown from plan
3. Verify A2A-009, A2A-004, A2A-002 completion status (blockers)
4. Execute implementation via `/sage.implement` or `/sage.stream`

**Expected Impact (Validated by COMPASS Research):**
- **Context Efficiency:** 80%+ reduction in context tokens
- **Performance:** 20%+ improvement in long-horizon task success rates
- **Cost Reduction:** 70-80% lower LLM API costs via test-time scaling
- **Retrieval Quality:** 95%+ precision (improved from 90% baseline)
- **Memory Coherence:** <5% contradictory retrievals
- **Scalability:** 1M+ memories per agent without degradation
