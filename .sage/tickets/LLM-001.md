# LLM-001: Multi-Provider LLM Client Service Implementation

**State:** UNPROCESSED
**Priority:** P0
**Type:** Epic
**Component:** llm-client-service
**Effort:** 15 days (2-3 weeks)
**Phase:** 1

## Description

Implement production-grade multi-provider LLM client supporting OpenAI, Anthropic Claude, and Google Gemini with unified interface, comprehensive metrics, and runtime model switching.

### Key Features

- **Multi-Provider Support**: OpenAI (gpt-4.1, gpt-5), Anthropic (claude-3-5), Gemini (gemini-2.0, gemini-1.5)
- **Unified Interface**: Single LLMRequest/LLMResponse model for all providers
- **Model Governance**: Enforce ALLOWED_MODELS configuration per CLAUDE.md
- **A2A Protocol Integration**: Full trace_id and source_agent propagation
- **Runtime Model Selection**: Config-based model switching (fast/balanced/premium tiers)
- **Streaming Support**: Async streaming for all providers
- **Comprehensive Metrics**: Prometheus instrumentation for latency, tokens, cost, errors

### Business Value

- Reduce vendor lock-in through provider abstraction
- Optimize costs (30% reduction target) through intelligent model selection
- Improve resilience through provider failover
- Enable A2A protocol-native LLM operations with full traceability

## Acceptance Criteria

- [ ] All three providers (OpenAI, Anthropic, Gemini) implemented and tested
- [ ] Unified LLMRequest/LLMResponse models in production
- [ ] Automatic provider selection operational based on model
- [ ] Model governance enforcing ALLOWED_MODELS (100% enforcement)
- [ ] A2A context propagation working (trace_id, source_agent, session_id)
- [ ] Streaming support functional for all providers
- [ ] Prometheus metrics instrumented and validated
- [ ] Runtime model selection via ModelSelector operational
- [ ] JSON-RPC methods registered (llm.complete, llm.models, llm.metrics)
- [ ] Unit tests achieving 90%+ coverage
- [ ] Integration tests passing with real provider APIs
- [ ] Abstraction overhead <5ms validated
- [ ] Documentation complete with usage examples
- [ ] All three providers tested in staging environment

## Dependencies

### External Dependencies
- OpenAI SDK (openai ^1.0.0)
- Anthropic SDK (anthropic ^0.40.0)
- Google GenAI SDK (google-genai ^0.2.0)
- HTTP client (httpx ^0.27.0)
- Metrics (prometheus-client ^0.21.0)

### Component Dependencies
- AgentCore JSON-RPC Handler (for method registration)
- AgentCore Configuration (for settings and API keys)
- None (new standalone service)

## Context

**Specification:** `docs/specs/llm-client-service/spec.md`
**Source Research:** `.docs/specs/SPEC_MULTI_PROVIDER_LLM_CLIENT.md`

### Component Structure

```
src/agentcore/a2a_protocol/
├── models/
│   └── llm.py                    # LLMRequest, LLMResponse, errors
├── services/
│   ├── llm_client_base.py        # Abstract LLMClient interface
│   ├── llm_client_openai.py      # OpenAI implementation
│   ├── llm_client_anthropic.py   # Anthropic implementation
│   ├── llm_client_gemini.py      # Gemini implementation
│   ├── llm_service.py            # LLMService facade + ProviderRegistry
│   ├── model_selector.py         # Runtime model selection
│   └── llm_jsonrpc.py            # JSON-RPC methods
├── metrics/
│   └── llm_metrics.py            # Prometheus metrics
```

### Configuration Required

```bash
# Environment variables
OPENAI_API_KEY=sk-...
ANTHROPIC_API_KEY=sk-ant-...
GEMINI_API_KEY=...

ALLOWED_MODELS=gpt-4.1-mini,claude-3-5-haiku-20241022,gemini-1.5-flash
LLM_DEFAULT_MODEL=gpt-4.1-mini
LLM_REQUEST_TIMEOUT=60
LLM_MAX_RETRIES=3
```

## Timeline

**Week 1: Core Infrastructure** (5 days)
- Data models and abstract interface
- OpenAI client implementation
- Anthropic client implementation

**Week 2: Multi-Provider Completion** (5 days)
- Gemini client implementation
- Provider registry
- LLM service facade
- Metrics instrumentation

**Week 3: Advanced Features** (5 days)
- Runtime model selection
- JSON-RPC integration
- Integration testing
- Documentation and deployment

## Progress

**Status:** Ready for planning with `/sage.plan`

**Next Steps:**
1. Run `/sage.plan` to generate detailed implementation plan
2. Run `/sage.tasks` to break down into SMART tasks
3. Execute implementation via `/sage.implement`

## Architecture

**Pattern:** Layered + Facade + Abstract Factory
- **Layered**: Separation between protocol (JSON-RPC), service (LLMService), and provider (clients)
- **Facade**: LLMService hides provider complexity behind simple interface
- **Abstract Factory**: ProviderRegistry creates appropriate client based on model

**Key Design Decisions:**
1. **Facade Pattern**: Single LLMService class wrapping all provider complexity
2. **Provider Selection**: ProviderRegistry maps models to providers at runtime (<1ms)
3. **Streaming Interface**: Unified AsyncIterator[str] for all providers
4. **Model Governance**: Validate ALLOWED_MODELS at service layer before provider selection
5. **A2A Context**: Propagate trace_id, source_agent via provider request headers

**Service Layers:**
```
JSON-RPC Layer (llm_jsonrpc.py)
    ↓
Service Layer (llm_service.py, ProviderRegistry, ModelSelector, LLMMetrics)
    ↓
Provider Layer (OpenAIClient, AnthropicClient, GeminiClient)
    ↓
External SDKs (AsyncOpenAI, anthropic.AsyncAnthropic, google.generativeai)
```

## Technology Stack

**Core:**
- Python 3.12+ (PEP 695 type syntax, async-first)
- FastAPI (existing AgentCore framework)
- Pydantic v2 (type-safe models with validation)
- asyncio (all I/O operations async)

**Provider SDKs:**
- openai ^1.0.0 (official SDK with async support)
- anthropic ^0.40.0 (official SDK with async support)
- google-generativeai ^0.2.0 (official Google SDK)

**Supporting:**
- httpx ^0.27.0 (already in AgentCore, async HTTP)
- prometheus-client ^0.21.0 (metrics instrumentation)
- structlog (existing AgentCore logging)

**CLAUDE.md Compliance:**
- Built-in generics: list[str], dict[str, Any], str | None
- Fail fast (no fallbacks or graceful degradation)
- Configuration-only model references
- ALLOWED_MODELS enforcement

## Updated Dependencies

### Cross-Component Dependencies

**Requires (Existing Components):**
- A2A Protocol JSON-RPC Handler (method registration infrastructure)
- Configuration System (Pydantic Settings for API keys and ALLOWED_MODELS)

**Enables (Future Enhancements):**
- Session Manager: LLM-powered conversation handling
- Task Manager: LLM-based task reasoning
- Agent Manager: LLM-driven capability assessment

**Integration Points:**
- JSON-RPC methods: llm.complete, llm.stream, llm.models, llm.metrics, llm.health
- Event Manager: Metrics published via event streams
- Prometheus: /metrics endpoint exposition

### Blocking Relationships

**None** - This is a standalone service with no blocking dependencies on other AgentCore components. Can be implemented independently.

**Enables:** Any future component requiring LLM capabilities should wait for this service to be completed.

## Risk Factors

**Critical Risks:**

1. **Provider API Changes** (Impact: High, Likelihood: Medium)
   - Mitigation: Version pinning, abstract interface, integration tests

2. **Rate Limit Exhaustion** (Impact: High, Likelihood: Medium)
   - Mitigation: Exponential backoff, retry logic, queuing, concurrency limits

3. **Cost Overruns** (Impact: High, Likelihood: Medium)
   - Mitigation: ALLOWED_MODELS enforcement, usage metrics, alerts

4. **Abstraction Overhead >5ms** (Impact: Medium, Likelihood: Low)
   - Mitigation: Async-first, minimal processing, performance tests in CI

5. **API Key Leakage** (Impact: High, Likelihood: Low)
   - Mitigation: Environment-only loading, log filtering, security tests

**Performance Targets:**
- Abstraction overhead: <5ms (p95)
- Provider selection: <1ms
- Time to first token: <500ms (streaming, p95)
- Concurrent requests: 1000+ per provider

## Implementation Plan Reference

**Detailed Plan:** `docs/specs/llm-client-service/plan.md` (PRP Format)

The plan includes:
- Complete architecture design with diagrams
- Technology stack rationale
- Code examples and patterns from existing AgentCore services
- Security considerations (API key handling, TLS, PII)
- Performance optimization strategies
- Comprehensive testing strategy (unit, integration, performance, security)
- 3-week phased implementation roadmap
- Risk management and mitigation strategies
- Error handling and edge cases
- Monitoring and alerting setup

## Notes

This is a foundational service for AgentCore that enables all agent operations requiring LLM capabilities. Priority P0 as Phase 1 component.

**Key Strategic Value:**
- Reduces vendor lock-in through provider abstraction
- Enables 30% cost reduction through intelligent model selection
- Provides A2A protocol-native LLM operations with distributed tracing
- Foundation for future intelligent agent capabilities
