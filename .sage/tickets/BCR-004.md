# BCR-004: Implement LLMClient Adapter

**State:** UNPROCESSED
**Priority:** P0
**Type:** Story

## Description

Create async LLM client adapter with support for stop sequences, token counting, and retry logic

## Acceptance Criteria

- [ ] LLMClient class with async `generate()` method
- [ ] Stop sequence support (`<answer>`, `<continue>`)
- [ ] Token counting via tiktoken integration
- [ ] Retry logic with exponential backoff (3 attempts: 1s, 2s, 4s)
- [ ] Circuit breaker pattern (open after 5 failures, close after 60s)
- [ ] Connection pooling via aiohttp
- [ ] Timeout handling (60s per call, configurable)
- [ ] Unit tests with mock LLM responses

## Dependencies

- #BCR-001 (parent epic)
- #BCR-003

## Context

**Specs:** docs/specs/bounded-context-reasoning/spec.md
**Plans:** docs/specs/bounded-context-reasoning/plan.md
**Tasks:** docs/specs/bounded-context-reasoning/tasks.md

## Effort

**Story Points:** 8
**Estimated Duration:** 4 days

## Progress

**Notes:** Generated from /sage.tasks command
