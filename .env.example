# =============================================================================
# AgentCore Environment Configuration Example
# =============================================================================
# Copy this file to .env and fill in your actual values

# =============================================================================
# Portkey AI Gateway Configuration
# =============================================================================
# Portkey provides unified LLM orchestration with routing, fallbacks, and caching
# Sign up at: https://portkey.ai

# Required: Your Portkey API key
PORTKEY_API_KEY=your-portkey-api-key-here

# Optional: Portkey gateway base URL (default: https://api.portkey.ai)
PORTKEY_BASE_URL=https://api.portkey.ai

# Optional: Portkey virtual key for provider routing
# Use virtual keys to route to different LLM providers without changing code
PORTKEY_VIRTUAL_KEY=

# =============================================================================
# LLM Model Configuration
# =============================================================================
# Default LLM model to use (via Portkey)
# Options: gpt-4.1, gpt-4.1-mini, gpt-5, gpt-5-mini, claude-3-sonnet, etc.
DEFAULT_LLM_MODEL=gpt-4.1

# Fallback models for resilience (comma-separated)
# Portkey will automatically fallback to these if primary model fails
LLM_FALLBACK_MODELS=gpt-4.1-mini,gpt-5-mini

# Default temperature for LLM requests (0.0 to 2.0)
LLM_TEMPERATURE=0.7

# Default maximum tokens for LLM responses
LLM_MAX_TOKENS=500

# =============================================================================
# LLM Performance & Reliability
# =============================================================================
# Timeout for LLM requests in seconds
LLM_TIMEOUT_SECONDS=30

# Maximum retry attempts for failed LLM requests
LLM_MAX_RETRIES=3

# Enable Portkey caching for cost reduction (true/false)
# Semantic caching can reduce costs by 50%+
LLM_CACHE_ENABLED=true

# =============================================================================
# LLM Client Service Configuration (AgentCore)
# =============================================================================
# Default LLM model for AgentCore LLM Client Service
# Must be one of: gpt-4.1-mini, gpt-5-mini, claude-3-5-haiku-20241022, gemini-1.5-flash
LLM_DEFAULT_MODEL=gpt-4.1-mini

# Request timeout for LLM API calls in seconds (must be >0)
LLM_REQUEST_TIMEOUT=60.0

# Maximum number of retry attempts for failed LLM requests (must be >=0)
# LLM_MAX_RETRIES=3

# Exponential backoff base for retry delays (must be >1)
LLM_RETRY_EXPONENTIAL_BASE=2.0

# =============================================================================
# Agent Runtime Configuration
# =============================================================================
# Host for agent runtime API
AGENT_RUNTIME_HOST=0.0.0.0

# Port for agent runtime API
AGENT_RUNTIME_PORT=8002

# Enable debug mode (true/false)
AGENT_RUNTIME_DEBUG=false

# =============================================================================
# Docker Configuration
# =============================================================================
# Docker daemon host
DOCKER_HOST=unix://var/run/docker.sock

# Docker network for agent containers
DOCKER_NETWORK=agent-runtime

# =============================================================================
# Logging Configuration
# =============================================================================
# Logging level: DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_LEVEL=INFO

# Use JSON logging format (true/false)
LOG_JSON=true

# =============================================================================
# Direct LLM Provider Keys (Optional)
# =============================================================================
# These are only needed if you're using direct provider integration
# When using Portkey, configure these in Portkey dashboard instead

# OpenAI API Key
OPENAI_API_KEY=

# Google Gemini API Key
GEMINI_API_KEY=

# Anthropic API Key
ANTHROPIC_API_KEY=

# =============================================================================
# A2A Protocol Configuration (from main application)
# =============================================================================
# Database connection
DATABASE_URL=postgresql+asyncpg://user:password@localhost:5432/agentcore

# Redis connection
REDIS_URL=redis://localhost:6379/0

# JWT configuration
JWT_SECRET_KEY=your-secret-key-here
JWT_ALGORITHM=HS256
JWT_EXPIRATION_HOURS=24

# A2A Protocol settings
A2A_PROTOCOL_VERSION=0.2
MAX_CONCURRENT_CONNECTIONS=1000
MESSAGE_TIMEOUT_SECONDS=30

# Monitoring
ENABLE_METRICS=true
