"""Comprehensive end-to-end integration tests for LLM Client Service.

This module provides comprehensive integration testing with real API calls.
Tests are skipped if API keys are not configured in the environment.

Test Coverage:
- End-to-end completion workflows across all providers
- Streaming functionality validation
- Concurrent request handling (100+ concurrent)
- A2A context propagation and tracing
- Error handling with transient failures
- Retry logic with rate limit scenarios
- Token usage tracking accuracy
- Response normalization consistency

Requirements:
- API keys configured in environment (OPENAI_API_KEY, ANTHROPIC_API_KEY, GEMINI_API_KEY)
- Network connectivity to provider APIs
- Sufficient rate limits for concurrent testing

Run with:
    uv run pytest tests/integration/services/test_llm_e2e_integration.py -v
    uv run pytest tests/integration/services/test_llm_e2e_integration.py -v -k test_e2e_completion
    uv run pytest tests/integration/services/test_llm_e2e_integration.py --timeout=300
"""

from __future__ import annotations

import asyncio
import os
import time
from typing import Any

import pytest

from agentcore.a2a_protocol.models.llm import LLMRequest
from agentcore.a2a_protocol.services.llm_service import LLMService, ProviderRegistry

# Timeout for integration tests (longer than unit tests)
pytestmark = pytest.mark.timeout(300)


@pytest.fixture(autouse=True)
def clear_provider_cache() -> None:
    """Clear Provider Registry instance cache before each test."""
    ProviderRegistry._instances.clear()
    yield
    ProviderRegistry._instances.clear()


@pytest.fixture
def llm_service() -> LLMService:
    """Create LLMService instance for testing."""
    return LLMService(timeout=90.0, max_retries=3)


class TestEndToEndCompletion:
    """End-to-end completion workflow tests with real API calls."""

    @pytest.mark.skipif(
        not os.getenv("OPENAI_API_KEY"),
        reason="OpenAI API key not configured",
    )
    @pytest.mark.asyncio
    async def test_e2e_completion_openai(self, llm_service: LLMService) -> None:
        """Test complete workflow with OpenAI provider."""
        request = LLMRequest(
            model="gpt-5-mini",
            messages=[
                {
                    "role": "user",
                    "content": "Say exactly 'Integration test successful' and nothing else.",
                }
            ],
            temperature=0.0,
            max_tokens=50,
            trace_id="test-e2e-openai-001",
            source_agent="test-agent",
        )

        response = await llm_service.complete(request)

        # Verify response structure
        assert response is not None
        assert response.content is not None
        assert len(response.content) > 0
        assert "successful" in response.content.lower()

        # Verify token usage tracking
        assert response.usage is not None
        assert response.usage.prompt_tokens > 0
        assert response.usage.completion_tokens > 0
        assert response.usage.total_tokens == (
            response.usage.prompt_tokens + response.usage.completion_tokens
        )

        # Verify model information
        assert response.model == "gpt-5-mini"

    @pytest.mark.skipif(
        not os.getenv("ANTHROPIC_API_KEY"),
        reason="Anthropic API key not configured",
    )
    @pytest.mark.asyncio
    async def test_e2e_completion_anthropic(self, llm_service: LLMService) -> None:
        """Test complete workflow with Anthropic provider."""
        request = LLMRequest(
            model="claude-haiku-4-5-20251001",
            messages=[
                {
                    "role": "user",
                    "content": "Say exactly 'Integration test successful' and nothing else.",
                }
            ],
            temperature=0.0,
            max_tokens=50,
            trace_id="test-e2e-anthropic-001",
            source_agent="test-agent",
        )

        response = await llm_service.complete(request)

        # Verify response structure
        assert response is not None
        assert response.content is not None
        assert len(response.content) > 0
        assert "successful" in response.content.lower()

        # Verify token usage tracking
        assert response.usage is not None
        assert response.usage.prompt_tokens > 0
        assert response.usage.completion_tokens > 0
        assert response.usage.total_tokens == (
            response.usage.prompt_tokens + response.usage.completion_tokens
        )

        # Verify model information
        assert response.model == "claude-haiku-4-5-20251001"

    @pytest.mark.skipif(
        not os.getenv("GEMINI_API_KEY"),
        reason="Google API key not configured",
    )
    @pytest.mark.asyncio
    async def test_e2e_completion_gemini(self, llm_service: LLMService) -> None:
        """Test complete workflow with Gemini provider."""
        request = LLMRequest(
            model="gemini-2.5-flash-lite",
            messages=[
                {
                    "role": "user",
                    "content": "Say exactly 'Integration test successful' and nothing else.",
                }
            ],
            temperature=0.0,
            max_tokens=50,
            trace_id="test-e2e-gemini-001",
            source_agent="test-agent",
        )

        response = await llm_service.complete(request)

        # Verify response structure
        assert response is not None
        assert response.content is not None
        assert len(response.content) > 0
        assert "successful" in response.content.lower()

        # Verify token usage tracking
        assert response.usage is not None
        assert response.usage.prompt_tokens > 0
        assert response.usage.completion_tokens > 0

        # Verify model information
        assert response.model == "gemini-2.5-flash-lite"


class TestEndToEndStreaming:
    """End-to-end streaming tests with real API calls."""

    @pytest.mark.skipif(
        not os.getenv("OPENAI_API_KEY"),
        reason="OpenAI API key not configured",
    )
    @pytest.mark.asyncio
    async def test_e2e_streaming_openai(self, llm_service: LLMService) -> None:
        """Test streaming functionality with OpenAI provider."""
        request = LLMRequest(
            model="gpt-5-mini",
            messages=[{"role": "user", "content": "Count from 1 to 5."}],
            temperature=0.0,
            max_tokens=50,
            stream=True,
            trace_id="test-stream-openai-001",
        )

        chunks = []
        async for chunk in llm_service.stream(request):
            chunks.append(chunk)

        # Verify we received multiple chunks
        assert len(chunks) > 0, "Should receive at least one chunk"

        # Verify chunks are strings
        assert all(isinstance(chunk, str) for chunk in chunks)

        # Verify complete response contains expected content
        full_response = "".join(chunks)
        assert len(full_response) > 0

    @pytest.mark.skipif(
        not os.getenv("ANTHROPIC_API_KEY"),
        reason="Anthropic API key not configured",
    )
    @pytest.mark.asyncio
    async def test_e2e_streaming_anthropic(self, llm_service: LLMService) -> None:
        """Test streaming functionality with Anthropic provider."""
        request = LLMRequest(
            model="claude-haiku-4-5-20251001",
            messages=[{"role": "user", "content": "Count from 1 to 5."}],
            temperature=0.0,
            max_tokens=50,
            stream=True,
            trace_id="test-stream-anthropic-001",
        )

        chunks = []
        async for chunk in llm_service.stream(request):
            chunks.append(chunk)

        # Verify we received multiple chunks
        assert len(chunks) > 0, "Should receive at least one chunk"

        # Verify chunks are strings
        assert all(isinstance(chunk, str) for chunk in chunks)

        # Verify complete response contains expected content
        full_response = "".join(chunks)
        assert len(full_response) > 0

    @pytest.mark.skipif(
        not os.getenv("GEMINI_API_KEY"),
        reason="Google API key not configured",
    )
    @pytest.mark.asyncio
    async def test_e2e_streaming_gemini(self, llm_service: LLMService) -> None:
        """Test streaming functionality with Gemini provider."""
        request = LLMRequest(
            model="gemini-2.5-flash-lite",
            messages=[{"role": "user", "content": "Count from 1 to 5."}],
            temperature=0.0,
            max_tokens=50,
            stream=True,
            trace_id="test-stream-gemini-001",
        )

        chunks = []
        async for chunk in llm_service.stream(request):
            chunks.append(chunk)

        # Verify we received multiple chunks
        assert len(chunks) > 0, "Should receive at least one chunk"

        # Verify chunks are strings
        assert all(isinstance(chunk, str) for chunk in chunks)

        # Verify complete response contains expected content
        full_response = "".join(chunks)
        assert len(full_response) > 0


class TestConcurrentRequests:
    """Concurrent request handling tests."""

    @pytest.mark.skipif(
        not os.getenv("OPENAI_API_KEY"),
        reason="OpenAI API key not configured",
    )
    @pytest.mark.asyncio
    async def test_concurrent_requests_10(self, llm_service: LLMService) -> None:
        """Test handling 10 concurrent requests (lightweight test)."""
        async def make_request(index: int) -> dict[str, Any]:
            request = LLMRequest(
                model="gpt-5-mini",
                messages=[
                    {
                        "role": "user",
                        "content": f"Say 'Request {index} complete' and nothing else.",
                    }
                ],
                temperature=0.0,
                max_tokens=20,
                trace_id=f"test-concurrent-{index:03d}",
            )
            start_time = time.time()
            response = await llm_service.complete(request)
            end_time = time.time()
            return {
                "index": index,
                "success": response is not None,
                "duration": end_time - start_time,
                "content": response.content if response else None,
            }

        # Execute 10 concurrent requests
        tasks = [make_request(i) for i in range(10)]
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # Verify all requests succeeded
        successes = [
            r for r in results if isinstance(r, dict) and r.get("success", False)
        ]
        assert len(successes) == 10, f"Expected 10 successes, got {len(successes)}"

        # Verify response content is correct
        for result in successes:
            assert isinstance(result["content"], str)
            assert len(result["content"]) > 0

        # Calculate average latency
        avg_latency = sum(r["duration"] for r in successes) / len(successes)
        print(f"\nAverage latency for 10 concurrent requests: {avg_latency:.2f}s")

    @pytest.mark.skipif(
        not os.getenv("OPENAI_API_KEY"),
        reason="OpenAI API key not configured",
    )
    @pytest.mark.asyncio
    @pytest.mark.slow
    async def test_concurrent_requests_100(self, llm_service: LLMService) -> None:
        """Test handling 100 concurrent requests (stress test).

        This test validates the system can handle production-level concurrent load.
        Marked as slow test - only run when explicitly requested.
        """
        async def make_request(index: int) -> dict[str, Any]:
            request = LLMRequest(
                model="gpt-5-mini",
                messages=[{"role": "user", "content": "Hi"}],
                temperature=0.0,
                max_tokens=5,
                trace_id=f"test-stress-{index:04d}",
            )
            start_time = time.time()
            try:
                response = await llm_service.complete(request)
                end_time = time.time()
                return {
                    "index": index,
                    "success": True,
                    "duration": end_time - start_time,
                }
            except Exception as e:
                end_time = time.time()
                return {
                    "index": index,
                    "success": False,
                    "duration": end_time - start_time,
                    "error": str(e),
                }

        # Execute 100 concurrent requests
        tasks = [make_request(i) for i in range(100)]
        results = await asyncio.gather(*tasks, return_exceptions=False)

        # Verify at least 95% success rate (allow for occasional rate limits)
        successes = [r for r in results if r.get("success", False)]
        success_rate = len(successes) / len(results)
        assert success_rate >= 0.95, f"Expected â‰¥95% success rate, got {success_rate:.1%}"

        # Calculate performance metrics
        durations = [r["duration"] for r in successes]
        avg_latency = sum(durations) / len(durations)
        p95_latency = sorted(durations)[int(len(durations) * 0.95)]
        p99_latency = sorted(durations)[int(len(durations) * 0.99)]

        print(f"\n100 Concurrent Requests Performance:")
        print(f"  Success rate: {success_rate:.1%}")
        print(f"  Average latency: {avg_latency:.2f}s")
        print(f"  P95 latency: {p95_latency:.2f}s")
        print(f"  P99 latency: {p99_latency:.2f}s")


class TestA2AContextPropagation:
    """A2A context propagation and tracing tests."""

    @pytest.mark.skipif(
        not os.getenv("OPENAI_API_KEY"),
        reason="OpenAI API key not configured",
    )
    @pytest.mark.asyncio
    async def test_trace_id_propagation(self, llm_service: LLMService) -> None:
        """Test trace_id is properly tracked throughout request lifecycle."""
        trace_id = "test-trace-propagation-001"

        request = LLMRequest(
            model="gpt-5-mini",
            messages=[{"role": "user", "content": "Hi"}],
            temperature=0.0,
            max_tokens=5,
            trace_id=trace_id,
        )

        response = await llm_service.complete(request)

        # Verify response contains trace information
        assert response is not None
        # Note: trace_id may not be in response model, but should be logged

    @pytest.mark.skipif(
        not os.getenv("OPENAI_API_KEY"),
        reason="OpenAI API key not configured",
    )
    @pytest.mark.asyncio
    async def test_source_agent_tracking(self, llm_service: LLMService) -> None:
        """Test source_agent is properly tracked for accountability."""
        request = LLMRequest(
            model="gpt-5-mini",
            messages=[{"role": "user", "content": "Hi"}],
            temperature=0.0,
            max_tokens=5,
            trace_id="test-agent-tracking-001",
            source_agent="test-agent-alpha",
        )

        response = await llm_service.complete(request)

        # Verify request completes successfully
        assert response is not None
        # Note: source_agent tracking verified via metrics and logs

    @pytest.mark.skipif(
        not os.getenv("OPENAI_API_KEY"),
        reason="OpenAI API key not configured",
    )
    @pytest.mark.asyncio
    async def test_session_id_propagation(self, llm_service: LLMService) -> None:
        """Test session_id is properly tracked for multi-turn conversations."""
        session_id = "test-session-001"

        # First turn
        request1 = LLMRequest(
            model="gpt-5-mini",
            messages=[{"role": "user", "content": "My name is Alice."}],
            temperature=0.0,
            max_tokens=20,
            trace_id="test-session-turn1",
            session_id=session_id,
        )

        response1 = await llm_service.complete(request1)
        assert response1 is not None

        # Second turn (same session)
        request2 = LLMRequest(
            model="gpt-5-mini",
            messages=[
                {"role": "user", "content": "My name is Alice."},
                {"role": "assistant", "content": response1.content},
                {"role": "user", "content": "What is my name?"},
            ],
            temperature=0.0,
            max_tokens=20,
            trace_id="test-session-turn2",
            session_id=session_id,
        )

        response2 = await llm_service.complete(request2)
        assert response2 is not None


class TestErrorHandling:
    """Error handling and recovery tests."""

    @pytest.mark.skipif(
        not os.getenv("OPENAI_API_KEY"),
        reason="OpenAI API key not configured",
    )
    @pytest.mark.asyncio
    async def test_invalid_model_error(self, llm_service: LLMService) -> None:
        """Test error handling for invalid model name."""
        request = LLMRequest(
            model="gpt-nonexistent-model",
            messages=[{"role": "user", "content": "Hi"}],
            trace_id="test-invalid-model-001",
        )

        with pytest.raises(Exception):  # Should raise ModelNotAllowedError or similar
            await llm_service.complete(request)

    @pytest.mark.skipif(
        not os.getenv("OPENAI_API_KEY"),
        reason="OpenAI API key not configured",
    )
    @pytest.mark.asyncio
    async def test_timeout_handling(self) -> None:
        """Test timeout handling with very short timeout."""
        llm_service_short_timeout = LLMService(timeout=0.001, max_retries=1)

        request = LLMRequest(
            model="gpt-5-mini",
            messages=[{"role": "user", "content": "Write a very long essay."}],
            max_tokens=1000,
            trace_id="test-timeout-001",
        )

        with pytest.raises(Exception):  # Should raise timeout error
            await llm_service_short_timeout.complete(request)


if __name__ == "__main__":
    pytest.main([__file__, "-v", "-s"])
